# Paola Optimizer Backend Status

## ‚úÖ All Optimizer Backends Installed and Ready

| Optimizer | Version | Status | Capabilities |
|-----------|---------|--------|--------------|
| **scipy** | 1.16.3 | ‚úÖ Ready | SLSQP, L-BFGS-B, COBYLA, Nelder-Mead, trust-constr |
| **pymoo** | 0.6.1.6 | ‚úÖ Ready | Multi-objective evolutionary algorithms (NSGA-II, NSGA-III) |
| **optuna** | 4.6.0 | ‚úÖ Ready | Bayesian optimization, TPE, CMA-ES, hyperparameter tuning |
| **cyipopt** | 1.4.1 | ‚úÖ Ready | IPOPT (Interior Point OPTimizer) for large-scale nonlinear optimization |

---

## LLM Backend Status

| Provider | Status | Configuration |
|----------|--------|---------------|
| **Qwen** | ‚úÖ Ready | `DASHSCOPE_API_KEY` set in `.env` |
| **Claude** | ‚úÖ Ready | `ANTHROPIC_API_KEY` set in `.env` |
| **OpenAI** | ‚úÖ Available | Requires `OPENAI_API_KEY` |
| **Ollama** | ‚è≥ Pending | Will use local DeepSeek-R1-70B when download completes |

---

## Quick Test

To verify Paola is working with all optimizers:

```bash
conda activate ml
python -m paola.cli
```

Then try an optimization problem:
```
> Optimize the Rosenbrock function with 2 variables
```

Paola should automatically select an appropriate optimizer (scipy, IPOPT, or pymoo) based on the problem characteristics.

---

## Optimizer Selection Guide

Paola automatically selects optimizers based on:

1. **Problem size**: Small (<10 vars) ‚Üí scipy, Large (>100 vars) ‚Üí IPOPT
2. **Constraints**: Linear/nonlinear constraints ‚Üí IPOPT or scipy SLSQP
3. **Multi-objective**: Multiple objectives ‚Üí pymoo (NSGA-II/III)
4. **Black-box/noisy**: Unknown gradients ‚Üí optuna or pymoo
5. **Hyperparameter tuning**: ML model tuning ‚Üí optuna

---

## Advanced Usage

### Force specific optimizer:
```python
# In Paola conversation
> Use IPOPT to optimize [problem]
> Use Bayesian optimization (optuna) for [problem]
> Use evolutionary algorithm (pymoo) for [problem]
```

### Enable deep thinking (Qwen):
```bash
python -m paola.cli --enable-thinking
```

This uses Qwen's reasoning mode for better mathematical understanding.

---

## Performance Notes

- **scipy**: Fast for smooth, gradient-based problems
- **IPOPT**: Best for large-scale constrained optimization
- **pymoo**: Excellent for multi-objective and discrete problems
- **optuna**: Superior for hyperparameter search and noisy objectives

---

## Next Steps

1. ‚úÖ All dependencies installed
2. ‚è≥ Wait for DeepSeek-R1-70B download to complete (for local LLM)
3. üéØ Start using Paola for optimization tasks!
