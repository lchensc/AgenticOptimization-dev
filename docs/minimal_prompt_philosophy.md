# Minimal Prompt Philosophy - PAOLA Agent

**Date**: 2025-12-14
**Issue**: Agent was too prescriptive, executing rigid workflows instead of being interactive
**Solution**: Minimal prompts that trust LLM intelligence

---

## Problem

Agent behavior was:
1. âŒ Too autonomous - executing entire workflows without asking user
2. âŒ Too rigid - following prescribed patterns
3. âŒ Not interactive - not clarifying or asking questions
4. âŒ Not following user's specific instructions

**Example**:
```
User: "create a 2D rosenbrock NLP"
Agent:
  - Creates benchmark problem
  - Creates NLP problem
  - Starts optimization run
  - Runs optimization
  - Finalizes run
  - Says "DONE"

User never asked for all these steps!
```

---

## Root Cause

### Prescriptive Prompts

**Before** (`paola/agent/prompts/optimization.py`):
```python
"""
You are an autonomous optimization agent.

**Instructions:**
1. Explain your reasoning before calling tools
2. Tool arguments must be valid JSON

Decide next action. Use tools or respond "DONE".
"""
```

**Problems**:
- "autonomous optimization agent" â†’ suggests it should work independently
- "Instructions" â†’ prescribes what to do
- "Decide next action" â†’ prescribes decision-making
- "respond DONE" â†’ prescribes when to stop

### Prescriptive Tool Hints

**Before** (system prompt):
```python
**Problem Formulation:**
- create_nlp_problem: Create NLP from registered Foundry evaluators (check foundry_list_evaluators first)
- create_benchmark_problem: Quick built-in test functions - use when no custom evaluator needed

**NOTE: When user says "registered evaluator", use foundry_list_evaluators + create_nlp_problem**
```

**Problems**:
- "(check foundry_list_evaluators first)" â†’ prescribes workflow
- "use when no custom evaluator needed" â†’ prescribes when to use
- "NOTE: When user says X, use Y" â†’ explicit decision tree

### Prescriptive Tool Descriptions

**Before** (`create_nlp_problem`):
```python
"""
IMPORTANT: This uses evaluators registered in Foundry (via foundry_store_evaluator).
Use foundry_list_evaluators() first to see available evaluators.
This is NOT for benchmark functions - use create_benchmark_problem for those.
"""
```

**Problems**:
- "Use X first" â†’ prescribes workflow
- "This is NOT for Y" â†’ prescribes negative rules

---

## Solution: Minimal Prompts

### Principle from CLAUDE.md

> **CRITICAL - Minimal Prompting**: Keep system prompts and tool schemas minimal. Trust the LLM's intelligence. Never add verbose guidance, formatting rules, or hand-holding without explicit permission. The agent must learn from experience, not from over-specified prompts.

### Applied Changes

#### 1. Main Agent Prompt

**After**:
```python
"""
You are PAOLA, an optimization assistant.

User request: {user_request}

Current state:
- Problem: {problem_status}
- Optimizer: {optimizer_status}
- Iteration: {iteration}
- Best objective: {best_obj}

Tools available:
{tool_list}
"""
```

**Changes**:
- âœ… "optimization assistant" (not "autonomous agent")
- âœ… "User request" (not "Goal")
- âœ… Minimal state (removed budget, cache, history, observations)
- âœ… Just facts, no instructions
- âœ… No "Decide", no "DONE", no workflows

#### 2. Tool List in System Prompt

**After**:
```python
**Problem Formulation:**
- create_benchmark_problem: Built-in analytical functions
- create_nlp_problem: NLP from registered evaluators

**Run Management:**
- start_optimization_run: Start new optimization run
...
```

**Changes**:
- âœ… Removed workflow hints: "(check X first)"
- âœ… Removed usage guidance: "use when..."
- âœ… Removed NOTE with decision tree
- âœ… Just tool name + brief description

#### 3. Tool Descriptions

**After** (`create_nlp_problem`):
```python
"""
Create Nonlinear Programming (NLP) problem from registered Foundry evaluators.

NLP standard form:
    minimize/maximize f(x)
    subject to:
      g_i(x) â‰¤ value
      h_j(x) = value
      x_lower â‰¤ x â‰¤ x_upper

Args:
    problem_id: Unique identifier
    objective_evaluator_id: Evaluator for objective function
    bounds: Design variable bounds
    ...

Returns:
    {success, problem_id, recommended_solvers, ...}
"""
```

**Changes**:
- âœ… Removed "IMPORTANT: Use X first"
- âœ… Removed "This is NOT for Y"
- âœ… Just describes WHAT it does, not WHEN to use it

**After** (`create_benchmark_problem`):
```python
"""
Create and register a built-in analytical benchmark problem.

Available benchmark functions:
- rosenbrock: Global minimum at (1,1,...,1)
- sphere: Global minimum at origin
...
"""
```

**Changes**:
- âœ… Removed "Use this for quick testing"
- âœ… Removed "For custom functions, use create_nlp_problem instead"
- âœ… Just describes WHAT it does

---

## What This Achieves

### 1. Trust the LLM

The LLM (qwen-plus, claude, gpt-4) is intelligent enough to:
- âœ… Understand what tools do from their descriptions
- âœ… Reason about which tool to use for a given task
- âœ… Ask user for clarification when unclear
- âœ… Follow user's specific instructions

### 2. Flexibility

Agent can now:
- âœ… Execute just what user asks (not entire workflows)
- âœ… Ask questions when unclear
- âœ… Choose different approaches based on context
- âœ… Learn from experience (not rigid patterns)

### 3. Interactive Behavior

Agent should now:
- âœ… Read user request carefully
- âœ… Execute what user asked for
- âœ… Stop and report back
- âœ… Wait for next instruction
- âœ… Ask if unsure

---

## Expected Behavior Changes

### Before (Rigid Workflow):

```
User: "create a 2D rosenbrock NLP"

Agent:
  ðŸ’­ I need to create NLP with rosenbrock...
  ðŸ”§ create_benchmark_problem(...)
  ðŸ”§ create_nlp_problem(...)
  ðŸ”§ start_optimization_run(...)
  ðŸ”§ run_scipy_optimization(...)
  ðŸ”§ finalize_optimization_run(...)
  âœ… DONE - Optimized successfully!
```

### After (Interactive):

```
User: "create a 2D rosenbrock NLP"

Agent:
  ðŸ’­ User wants to create an NLP problem...
  ðŸ”§ foundry_list_evaluators()

  I found rosenbrock_eval in Foundry. I'll create an NLP problem with:
  - Objective: rosenbrock_eval
  - Dimension: 2D
  - Bounds: [-5, 10] for each dimension (typical for Rosenbrock)

  ðŸ”§ create_nlp_problem(
      problem_id="rosenbrock_2d_nlp",
      objective_evaluator_id="rosenbrock_eval",
      bounds=[[-5, 10], [-5, 10]]
  )

  âœ… Created NLP problem "rosenbrock_2d_nlp"
  Recommended solvers: SLSQP, L-BFGS-B

  [Waits for user's next instruction]
```

**Key differences**:
- âœ… Only does what user asked
- âœ… Explains what it's doing
- âœ… Stops after completing the request
- âœ… Doesn't assume user wants to optimize immediately

---

## Testing

### Test 1: Minimal Request

```
User: "list registered evaluators"

Expected:
Agent:
  ðŸ”§ foundry_list_evaluators()

  Available evaluators:
  - rosenbrock_eval
  - sphere_eval

  [Done - waits for next instruction]
```

### Test 2: Ambiguous Request

```
User: "create rosenbrock problem"

Expected:
Agent:
  ðŸ’­ "create rosenbrock problem" could mean:
     1. Built-in benchmark (create_benchmark_problem)
     2. From registered evaluator (create_nlp_problem)

  Let me check if rosenbrock is registered...

  ðŸ”§ foundry_list_evaluators()

  Found rosenbrock_eval in Foundry. I'll use that.

  ðŸ”§ create_nlp_problem(...)

  [Or if not found: create_benchmark_problem(...)]
```

### Test 3: Multi-Step Request

```
User: "create and optimize rosenbrock in 2D with SLSQP"

Expected:
Agent:
  ðŸ’­ User wants me to create AND optimize...

  ðŸ”§ foundry_list_evaluators()
  ðŸ”§ create_nlp_problem(...)
  ðŸ”§ run_scipy_optimization(algorithm="SLSQP", ...)

  Optimization completed:
  - Final design: [0.999, 0.999]
  - Final objective: 1.8e-8

  [Done - reports back]
```

---

## Comparison: Before vs After

| Aspect | Before (Prescriptive) | After (Minimal) |
|--------|----------------------|-----------------|
| **Agent identity** | "autonomous optimization agent" | "optimization assistant" |
| **Prompt length** | ~30 lines | ~10 lines |
| **Instructions** | Explicit (1, 2, 3...) | None |
| **Tool hints** | "(check X first)", "use when..." | Tool name + brief description |
| **Tool descriptions** | "IMPORTANT: Use X first", "NOT for Y" | Just describes capabilities |
| **Workflows** | Prescribed patterns | LLM decides |
| **Behavior** | Executes full workflows | Executes user request |
| **Flexibility** | Rigid | Flexible |

---

## Files Modified

1. **`paola/agent/prompts/optimization.py`**
   - Main agent prompt: 30 lines â†’ 10 lines
   - Tool list: Removed workflow hints and NOTEs
   - Removed budget, cache, history, observations from state

2. **`paola/tools/evaluator_tools.py`**
   - `create_nlp_problem`: Removed prescriptive guidance
   - `create_benchmark_problem`: Removed usage hints

**Total reduction**: ~50 lines of prescriptive text removed

---

## Principles Going Forward

### DO:
- âœ… Describe WHAT tools do (capabilities, requirements, outputs)
- âœ… Trust LLM intelligence
- âœ… Keep prompts minimal
- âœ… Use factual language
- âœ… Show tool schemas clearly

### DON'T:
- âŒ Prescribe WHEN to use tools ("use X when...")
- âŒ Prescribe HOW to use tools ("first do X, then Y")
- âŒ Add workflow hints ("check X first")
- âŒ Add decision trees ("if user says X, do Y")
- âŒ Over-specify behavior

### Quote from CLAUDE.md:
> "The agent must learn from experience, not from over-specified prompts."

---

**Status**: IMPLEMENTED âœ…

The agent now has minimal prompts that trust LLM intelligence. It should behave more like Claude Code - interactive, flexible, following specific user instructions rather than executing rigid workflows.
