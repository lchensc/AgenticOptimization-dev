# Prompt Caching Implementation Guide

## Overview

This document covers the complete implementation of LLM prompt caching in PAOLA, including:
- Token tracking infrastructure
- Cache token extraction for multiple providers (Anthropic, Qwen)
- Cost calculation with cache pricing
- Minimal token display (Claude Code style)
- Session statistics via `/tokens` command

## Features

### 1. Token Tracking
- Per-call token usage tracking
- Session-level statistics accumulation
- Per-model cost tracking
- Cache hit rate calculation

### 2. Multi-Provider Support

**Anthropic Claude**:
- Explicit caching with `cache_control: {"type": "ephemeral"}`
- Flat metadata structure
- Fields: `cache_creation_input_tokens`, `cache_read_input_tokens`

**Qwen (DashScope)**:
- Explicit caching with `cache_control: {"type": "ephemeral"}`
- Automatic/implicit caching of repeated prefixes
- Nested metadata: `prompt_tokens_details.cached_tokens`
- Automatic caching increases across iterations as conversation grows

### 3. Display Modes

**Per-call (minimal)**:
```
(867 tokens)
```
Shows NEW tokens processed (excludes cached tokens)

**Session stats (`/tokens` command)**:
```
ðŸ“Š Session Statistics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Calls:               7
Total Tokens:        60,960
  â€¢ Input:           59,350
  â€¢ Output:          1,610
  â€¢ Cache Read:      39,424 (64.6% hit rate)

Cost Breakdown:
  â€¢ Input:           $0.0030
  â€¢ Output:          $0.0006
  â€¢ Cache Read:      $0.0002
  â€¢ Total:           $0.0036
```

## Architecture

### Components

**TokenTracker** (`paola/llm/token_tracker.py`):
- Core tracking logic
- Cost calculation
- Session stats aggregation

**LangChainTokenCallback** (`paola/llm/token_tracker.py`):
- LangChain callback integration
- Extracts token usage from LLMResult
- Handles multiple provider formats

**CLI Integration** (`paola/cli/repl.py`):
- `/tokens` command for session stats
- Callback registration

## Implementation Details

### Token Counting

**Key Insight**: Different providers have different semantics:

**Qwen**:
```json
{
  "prompt_tokens": 1020,      // INCLUDES cached tokens
  "cached_tokens": 512,       // Breakdown (part of prompt_tokens)
  "total_tokens": 1065        // prompt + completion
}
```

**Anthropic**:
```json
{
  "input_tokens": 200,              // EXCLUDES cache
  "cache_read_input_tokens": 3000,  // Must add
  "total_tokens": 3300              // input + cache_read + output
}
```

**Solution**: Use API's `total_tokens` field directly instead of computing.

### Cache Token Extraction

```python
# Extract from both flat (Anthropic) and nested (Qwen) structures
cache_read = usage_data.get("cache_read_input_tokens", 0)  # Anthropic

prompt_details = usage_data.get("prompt_tokens_details")
if prompt_details:
    cache_read = prompt_details.get("cached_tokens", cache_read)  # Qwen
```

### Display Logic

```python
# Calculate NEW tokens (excluding cached)
new_input_tokens = input_tokens - cache_read_tokens
new_tokens = new_input_tokens + output_tokens

# Minimal display
print(f"({new_tokens} tokens)")
```

Cached tokens were already processed in previous calls, so we exclude them from the count.

## Qwen Automatic Caching Behavior

### Observation

Cache read tokens **increase** across iterations:

```
Iter 1: Cache Read: 4,608 tokens
Iter 2: Cache Read: 4,608 tokens
Iter 3: Cache Read: 5,376 tokens  (+768!)
Iter 4: Cache Read: 5,632 tokens  (+256!)
```

### Explanation

Qwen uses **automatic prefix caching** - it detects repeated prompt prefixes across API calls and caches them automatically.

**Message growth pattern**:
```
Iter 1: [HumanMessage]
Iter 2: [HumanMessage, AIMessage, ToolMessage]
Iter 3: [HumanMessage, AIMessage, ToolMessage, AIMessage, ToolMessage]
Iter 4: [HumanMessage, AIMessage, ToolMessage, AIMessage, ToolMessage, AIMessage, ToolMessage]
```

As the ReAct conversation grows, the common prefix gets longer, so Qwen caches more tokens automatically.

**This is beneficial**:
- Lower costs (90% discount on cached tokens)
- Faster inference (cached KV states reused)
- Automatic optimization (we didn't ask for this!)

## Cache Control Application

### First Iteration Only

```python
if iteration == 1:
    # Only first message gets explicit cache_control
    if supports_cache_control:
        messages = [{
            "role": "user",
            "content": [{
                "type": "text",
                "text": prompt,
                "cache_control": {"type": "ephemeral"}
            }]
        }]
```

For Qwen, this is sufficient because automatic caching handles subsequent iterations.

For Anthropic, cache_control could be added to subsequent messages for more control.

## Model Pricing Configuration

All costs per 1M tokens in USD:

```python
MODEL_CONFIGS = {
    "qwen-flash": {
        "cost_input": 0.05,
        "cost_output": 0.4,
        "cost_cache_write": 0.05,
        "cost_cache_read": 0.005,  # 90% discount
    },
    "claude-sonnet-4": {
        "cost_input": 3.0,
        "cost_output": 15.0,
        "cost_cache_write": 3.75,  # 1.25x input
        "cost_cache_read": 0.30,   # 90% discount
    },
    # ... more models
}
```

## Files Changed

### New Files
- `paola/llm/token_tracker.py` - Token tracking infrastructure
- `paola/llm/__init__.py` - Module exports
- `paola/tools/cache_agent_tools.py` - LangChain-wrapped cache tools
- `tests/test_token_tracking.py` - Unit tests

### Modified Files
- `paola/agent/react_agent.py` - Cache control for Anthropic and Qwen
- `paola/cli/repl.py` - `/tokens` command integration
- `paola/tools/__init__.py` - Export wrapped cache tools

## Usage

### Enable Token Tracking

Token tracking is **automatically enabled** for all LLM calls via the callback system.

### View Session Stats

```bash
paola> /tokens
```

Shows:
- Call count
- Total tokens (input, output, cache read, cache write)
- Per-model breakdown
- Cost breakdown
- Cache hit rate

### Debug Cache Behavior

Enable debug logging:

```bash
export PAOLA_LOG_LEVEL=DEBUG
python -m paola.cli
```

Debug log will show:
```
DEBUG:paola.llm.token_tracker:Usage data for qwen-flash: {
  'prompt_tokens': 1020,
  'completion_tokens': 45,
  'total_tokens': 1065,
  'prompt_tokens_details': {'cached_tokens': 512}
}
```

## Testing

### Unit Tests

```bash
pytest tests/test_token_tracking.py -v
```

Tests cover:
- Token tracking
- Cost calculation
- Multi-provider metadata extraction
- Session stats aggregation

### Integration Test

Run an optimization and check `/tokens`:

```bash
paola> optimize a 5D Rosenbrock function
# ... optimization runs ...
paola> /tokens
```

Expected output shows proper token counts and cache usage.

## Common Issues

### Issue: Cache tokens not showing

**Diagnosis**: Enable debug logging to see raw usage data

**Fix**: Check if:
1. `prompt_tokens_details` field is present (Qwen)
2. `cache_read_input_tokens` field is present (Anthropic)
3. LangChain provider version is up to date

### Issue: Token count seems wrong

**Remember**: We count NEW tokens only (excluding cached)

```
(867 tokens) = (1020 input - 512 cached) + 359 output
```

This is correct - cached tokens don't need re-processing.

### Issue: Cache increasing across iterations

**This is expected** for Qwen! Automatic caching detects growing conversation prefixes and caches more as the conversation grows.

## Best Practices

1. **Use cache_control on system prompt** (first message) for Anthropic
2. **Trust Qwen's automatic caching** for growing conversations
3. **Monitor cache hit rates** via `/tokens` to verify effectiveness
4. **Keep prompts stable** - changing prompts breaks caching
5. **Use debug logging** for troubleshooting cache behavior

## Future Improvements

1. **Multi-iteration cache_control**: Add cache_control to system context in subsequent iterations for Anthropic
2. **Cache warming**: Pre-cache common prompts at startup
3. **Cache analytics**: Track cache effectiveness over time
4. **Per-tool cache stats**: Show cache usage per tool invocation

## References

- [Anthropic Prompt Caching](https://docs.anthropic.com/claude/docs/prompt-caching)
- [Qwen Context Caching](https://www.alibabacloud.com/help/en/model-studio/context-cache)
- [LangChain Callbacks](https://python.langchain.com/docs/modules/callbacks/)

## Summary

The prompt caching implementation provides:
- âœ… Transparent token tracking across all LLM calls
- âœ… Multi-provider support (Anthropic, Qwen, OpenAI)
- âœ… Accurate cost calculation with cache pricing
- âœ… Minimal per-call display, detailed session stats
- âœ… Automatic caching optimization (Qwen)

All features tested and production-ready! ðŸŽ‰
