# pymoo Optimizer Skill
# Paola Skills v1.0

# === METADATA (Level 1 - Always indexed) ===
name: pymoo
version: "1.0.0"
category: optimizers

description: >
  pymoo is a comprehensive multi-objective optimization library providing
  single-objective (GA, DE, PSO, CMA-ES) and multi-objective (NSGA-II, NSGA-III,
  MOEA/D, AGE-MOEA) evolutionary algorithms with full Pareto front support.

when_to_use:
  - Multi-objective optimization requiring Pareto fronts
  - Population-based global search (escaping local minima)
  - Black-box optimization without gradients
  - Noisy or discontinuous objective functions
  - Problems with many local optima
  - Constrained optimization with complex constraint landscapes
  - When hypervolume or IGD metrics are needed

when_not_to_use:
  - Smooth convex problems - use gradient-based (SciPy, IPOPT)
  - Very high dimensional (1000+ variables) - too slow
  - Need reproducible exact solutions (stochastic nature)
  - Limited function evaluation budget (<100 evals) - use local methods
  - Problems requiring warm-start with Lagrange multipliers

keywords:
  - pymoo
  - evolutionary
  - genetic algorithm
  - GA
  - DE
  - differential evolution
  - PSO
  - particle swarm
  - CMA-ES
  - NSGA-II
  - NSGA-III
  - MOEA/D
  - AGE-MOEA
  - multi-objective
  - Pareto
  - population-based
  - global optimization

# === OVERVIEW (Level 2 - Loaded on skill activation) ===
overview: |
  ## pymoo Optimization Overview

  pymoo provides state-of-the-art evolutionary algorithms for both single-objective
  and multi-objective optimization. All algorithms are population-based and
  derivative-free, making them ideal for global search.

  ### Algorithm Selection Guide

  #### Single-Objective Algorithms

  | Algorithm | Best For | Key Strength |
  |-----------|----------|--------------|
  | GA | General purpose | Flexible operators, handles constraints |
  | DE | Continuous, smooth | Fast convergence, few parameters |
  | PSO | Continuous, exploration | Natural parallelism, simple tuning |
  | CMA-ES | Continuous, noisy | Self-adapting, robust |
  | ES | High-dimensional | Evolution strategies |
  | BRKGA | Combinatorial | Biased random-key encoding |

  #### Multi-Objective Algorithms

  | Algorithm | Best For | Objectives |
  |-----------|----------|------------|
  | NSGA-II | 2-3 objectives | Most common, fast non-dominated sorting |
  | NSGA-III | 4+ objectives | Reference point based |
  | MOEA/D | Many objectives | Decomposition approach |
  | AGE-MOEA | Any | Adaptive geometry estimation |
  | SMS-EMOA | 2-3 objectives | Hypervolume-based selection |
  | C-TAEA | Constrained | Constraint handling focus |

  ### Multi-Objective Metrics

  pymoo computes standard MO metrics:

  - **Hypervolume**: Volume dominated by Pareto front (higher is better)
  - **IGD**: Inverted Generational Distance (lower is better)
  - **Spread**: Diversity of solutions on front

  ### Integration with Paola Recording API

  ```python
  import paola
  from pymoo.algorithms.moo.nsga2 import NSGA2
  from pymoo.core.problem import Problem
  from pymoo.optimize import minimize as pymoo_minimize
  import numpy as np
  import json

  # Get RecordingObjective (problem_id is INTEGER from list_problems/create_nlp_problem)
  f = paola.objective(problem_id=7, goal="Multi-objective optimization with NSGA-II")

  # Wrap for pymoo
  class PaolaProblem(Problem):
      def __init__(self, recording_obj, n_var, xl, xu, n_obj):
          super().__init__(n_var=n_var, n_obj=n_obj, xl=np.array(xl), xu=np.array(xu))
          self.f = recording_obj

      def _evaluate(self, X, out, *args, **kwargs):
          F = []
          for x in X:
              result = self.f(x)  # f is callable, records evaluation
              F.append(result if isinstance(result, (list, np.ndarray)) else [result])
          out["F"] = np.array(F)

  # Set up problem and algorithm
  problem = PaolaProblem(f, n_var=2, xl=[-5, -5], xu=[5, 5], n_obj=2)
  algorithm = NSGA2(pop_size=100)

  # Run optimization
  res = pymoo_minimize(problem, algorithm, ("n_gen", 100), seed=42, verbose=False)

  # Access Pareto front
  print(f"Pareto front shape: {res.F.shape}")
  print(f"Best solutions: {res.X[:5]}")

  summary = paola.checkpoint(f, script="...", reasoning="NSGA-II for bi-objective MOO")
  print(json.dumps(summary))
  ```

  **Key points:**
  - `problem_id` is an INTEGER from `list_problems()` or `create_nlp_problem()`
  - Call `pymoo_minimize(...)` directly - NOT `f.run_optimization(...)`
  - `n_obj` is set in the Problem class, matching evaluator's output count
  - Pareto results: `res.F` (objectives), `res.X` (decision variables)
  - For constraints: override `_evaluate` to set `out["G"]` for constraint values

  ### Termination Criteria

  | Criterion | Description |
  |-----------|-------------|
  | n_gen | Maximum generations |
  | n_eval | Maximum evaluations |
  | time | Wall clock time limit |
  | ftol | Function value tolerance |
  | xtol | Design variable tolerance |

# === RESOURCES (Level 3 - Loaded on demand) ===
resources:
  options: options.yaml

# === RELATIONSHIPS ===
related_skills:
  - scipy
  - optuna

# === PAOLA INTEGRATION (Unique to Paola Skills) ===
paola:
  optimizer_name: pymoo
  backend: PymooBackend
  requires_gradient: false
  supports_constraints: true
  supports_multiobjective: true
  supports_warm_start: true  # Via initial population

  algorithms:
    # Single-objective
    GA:
      family: evolutionary
      objectives: single
      best_for: "General-purpose genetic algorithm"
      key_params: [pop_size, n_gen, crossover, mutation]
    DE:
      family: evolutionary
      objectives: single
      best_for: "Continuous optimization, fast convergence"
      key_params: [pop_size, n_gen, F, CR, variant]
    PSO:
      family: evolutionary
      objectives: single
      best_for: "Exploration, natural parallelism"
      key_params: [pop_size, n_gen, w, c1, c2]
    CMA-ES:
      family: evolutionary
      objectives: single
      best_for: "Noisy objectives, self-adapting"
      key_params: [pop_size, n_gen, sigma]
    ES:
      family: evolutionary
      objectives: single
      best_for: "Evolution strategies"
      key_params: [pop_size, n_gen, n_offspring]
    BRKGA:
      family: evolutionary
      objectives: single
      best_for: "Combinatorial, biased random-key"
      key_params: [pop_size, n_gen, n_elites, n_mutants]

    # Multi-objective
    NSGA-II:
      family: multiobjective
      objectives: 2-3
      best_for: "Standard multi-objective, fast"
      key_params: [pop_size, n_gen, crossover, mutation]
    NSGA-III:
      family: multiobjective
      objectives: 4+
      best_for: "Many objectives, reference points"
      key_params: [pop_size, n_gen, ref_dirs, crossover, mutation]
    MOEA/D:
      family: multiobjective
      objectives: many
      best_for: "Decomposition-based, many objectives"
      key_params: [pop_size, n_gen, ref_dirs, n_neighbors]
    AGE-MOEA:
      family: multiobjective
      objectives: any
      best_for: "Adaptive geometry estimation"
      key_params: [pop_size, n_gen]
    SMS-EMOA:
      family: multiobjective
      objectives: 2-3
      best_for: "Hypervolume-based selection"
      key_params: [pop_size, n_gen]
    C-TAEA:
      family: multiobjective
      objectives: any
      best_for: "Constrained multi-objective"
      key_params: [pop_size, n_gen, ref_dirs]

  graph_integration:
    warm_start_support: |
      pymoo algorithms can warm-start from previous populations via
      `initial_population` in config. This enables graph-based progressive
      refinement across multiple optimization nodes.

    typical_usage: |
      - Global search: Start with GA or DE, wide exploration
      - Refinement: Use smaller population or local search
      - Multi-objective: NSGA-II for 2-3 obj, NSGA-III for 4+

  learning:
    track_options:
      - algorithm
      - pop_size
      - n_gen
      - crossover
      - mutation
    success_indicators:
      - termination_reason_contains: "ftol"
      - termination_reason_contains: "xtol"
    failure_patterns:
      - pattern: "No feasible solution"
        likely_cause: "Constraints too tight or conflicting"
        suggested_action: "Relax constraints or check definitions"
      - pattern: "Maximum generations reached"
        likely_cause: "Insufficient generations for convergence"
        suggested_action: "Increase n_gen or pop_size"
