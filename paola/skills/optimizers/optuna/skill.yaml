# Optuna Optimizer Skill
# Paola Skills v1.0

# === METADATA (Level 1 - Always indexed) ===
name: optuna
version: "1.0.0"
category: optimizers

description: >
  Optuna is a hyperparameter optimization framework with define-by-run API,
  supporting multiple samplers (TPE, CMA-ES, GP, NSGA-II/III, QMC), automated
  pruning, and multi-objective optimization. Particularly strong for HPO, NAS,
  and black-box optimization.

when_to_use:
  - Black-box optimization (no gradient available)
  - Hyperparameter optimization (HPO)
  - Neural architecture search (NAS)
  - Multi-objective optimization (Pareto front)
  - Problems with mixed continuous/categorical/conditional parameters
  - When early stopping (pruning) can save computation
  - Parallel/distributed optimization

when_not_to_use:
  - Problems where gradient is available and cheap - use SciPy/IPOPT
  - Problems with many equality constraints - use IPOPT/SLSQP
  - Very small problems (< 5 variables) with smooth objective - gradient methods faster
  - When you need exact constraint satisfaction - Optuna handles constraints softly

keywords:
  - optuna
  - bayesian optimization
  - TPE
  - CMA-ES
  - gaussian process
  - hyperparameter
  - pruning
  - multi-objective
  - NSGA-II
  - black-box
  - derivative-free

# === OVERVIEW (Level 2 - Loaded on skill activation) ===
overview: |
  ## Optuna Overview

  Optuna is a next-generation hyperparameter optimization framework with a
  **define-by-run** API that allows dynamic, conditional search spaces.

  ### Key Concepts

  **Study**: An optimization session containing multiple trials
  **Trial**: A single evaluation of the objective function
  **Sampler**: Algorithm for suggesting parameter values
  **Pruner**: Algorithm for early stopping unpromising trials

  ### Sampler Selection Guide

  | Problem Type | Recommended Sampler | Why |
  |--------------|---------------------|-----|
  | General HPO, unknown structure | TPE | Robust default, handles conditionals |
  | Continuous, few params (< 10) | GP | Best Bayesian modeling |
  | Continuous, many params | CMA-ES | Efficient for high-dim continuous |
  | Multi-objective | NSGA-II or NSGA-III | Pareto-optimal solutions |
  | Systematic coverage | QMC (Sobol/Halton) | Low-discrepancy sampling |
  | Baseline comparison | Random | No assumptions |
  | Exhaustive small space | Grid | When space is enumerable |

  ### Sampler Capabilities Matrix

  | Sampler | Continuous | Categorical | Conditional | Multi-Obj |
  |---------|------------|-------------|-------------|-----------|
  | TPE | Yes | Yes | Yes | No* |
  | CMA-ES | Yes | No | No | No |
  | GP | Yes | Yes | Limited | No |
  | NSGA-II | Yes | Yes | Yes | Yes |
  | NSGA-III | Yes | Yes | Yes | Yes |
  | QMC | Yes | No | No | No |
  | Random | Yes | Yes | Yes | Yes |

  *TPE can be used with MOTPESampler for multi-objective

  ### Pruner Selection Guide

  | Use Case | Recommended Pruner | Why |
  |----------|-------------------|-----|
  | Expensive evaluations | Hyperband | Aggressive early stopping |
  | Fair comparison needed | SuccessiveHalving | Consistent resource allocation |
  | Simple, robust | Median | Prune below-median trials |
  | Known threshold | Threshold | Stop if worse than target |
  | Noisy evaluations | Patient + Median | Wait before pruning |

  ### Integration with Paola Recording API

  **Single-objective optimization:**
  ```python
  import paola
  import optuna
  import json

  # Get RecordingObjective (problem_id is INTEGER from list_problems/create_nlp_problem)
  f = paola.objective(problem_id=7, goal="Black-box optimization with TPE")

  # Define bounds (from get_problem_info or manually)
  bounds = [(-5, 5), (-5, 5)]

  def objective(trial):
      x = [trial.suggest_float(f"x{i}", lb, ub) for i, (lb, ub) in enumerate(bounds)]
      return f(x)  # f is callable, records evaluation

  study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42))
  study.optimize(objective, n_trials=100)

  summary = paola.checkpoint(f, script="...", reasoning="TPE for black-box")
  print(json.dumps(summary))
  ```

  **Multi-objective (MOO) optimization:**
  ```python
  # For Pareto front with 2 objectives
  def moo_objective(trial):
      x = [trial.suggest_float(f"x{i}", lb, ub) for i, (lb, ub) in enumerate(bounds)]
      result = f(x)  # Returns array [f1, f2] for MOO evaluator
      return result[0], result[1]

  study = optuna.create_study(
      directions=["minimize", "minimize"],
      sampler=optuna.samplers.NSGAIISampler(seed=42)
  )
  study.optimize(moo_objective, n_trials=200)

  # Access Pareto front
  pareto_trials = study.best_trials
  ```

  **Key points:**
  - `problem_id` is an INTEGER from `list_problems()` or `create_nlp_problem()`
  - Call `study.optimize(...)` directly - NOT `f.run_optimization(...)`
  - Bounds come from problem definition (use `get_problem_info()`)
  - For MOO: use `NSGAIISampler` or `NSGAIIISampler` with `directions=["minimize", "minimize"]`
  - Warm-start: enqueue known good points via `study.enqueue_trial()`

# === RESOURCES (Level 3 - Loaded on demand) ===
resources:
  options: options.yaml

# === RELATIONSHIPS ===
related_skills:
  - scipy
  - ipopt

# === PAOLA INTEGRATION (Unique to Paola Skills) ===
paola:
  optimizer_name: optuna
  backend: OptunaBackend
  requires_gradient: false
  supports_constraints: soft  # Via constraints_func
  supports_warm_start: true   # CMA-ES source_trials, enqueue_trial

  samplers:
    TPE:
      continuous: true
      categorical: true
      conditional: true
      best_for: "General HPO, conditional spaces, unknown problem structure"
      key_options:
        - n_startup_trials
        - multivariate
        - constant_liar
    CMA-ES:
      continuous: true
      categorical: false
      conditional: false
      best_for: "Continuous optimization, evolutionary strategy"
      key_options:
        - sigma0
        - restart_strategy
        - source_trials
    GP:
      continuous: true
      categorical: true
      conditional: limited
      best_for: "Few parameters, need uncertainty estimates"
      key_options:
        - n_startup_trials
        - deterministic_objective
    NSGA-II:
      continuous: true
      categorical: true
      conditional: true
      multi_objective: true
      best_for: "Multi-objective optimization, Pareto front"
      key_options:
        - population_size
        - crossover_prob
        - mutation_prob
    NSGA-III:
      continuous: true
      categorical: true
      conditional: true
      multi_objective: true
      best_for: "Many-objective optimization (3+ objectives)"
      key_options:
        - population_size
        - reference_points
    QMC:
      continuous: true
      categorical: false
      conditional: false
      best_for: "Systematic space coverage, quasi-random exploration"
      key_options:
        - qmc_type
        - scramble
    Random:
      continuous: true
      categorical: true
      conditional: true
      best_for: "Baseline comparison, simple exploration"
      key_options:
        - seed

  pruners:
    MedianPruner:
      best_for: "Simple, robust early stopping"
      note: "Prune if intermediate value below median of completed trials"
    HyperbandPruner:
      best_for: "Expensive evaluations, aggressive pruning"
      note: "Combines SuccessiveHalving with different budgets"
    SuccessiveHalvingPruner:
      best_for: "Fair comparison, consistent resource allocation"
      note: "Asynchronous SHA for parallel execution"
    ThresholdPruner:
      best_for: "When target value is known"
      note: "Prune if worse than threshold"
    PatientPruner:
      best_for: "Noisy evaluations"
      note: "Wraps another pruner with tolerance"

  graph_integration:
    warm_start_support: |
      CMA-ES supports warm-starting via `source_trials` parameter.
      For other samplers, use `study.enqueue_trial()` to suggest
      known good points from parent node.

    multi_stage_strategy: |
      Graph enables powerful multi-stage optimization:
      - Node 1: Wide exploration with TPE or Random (n_trials=50)
      - Node 2: Local refinement with CMA-ES from best region
      - Node 3: Final polish with increased budget

    search_space_adaptation: |
      To adapt search space between optimizations:
      - Register a new problem with different bounds using `create_nlp_problem`
      - Create a new graph for the new problem
      - Use warm_start to transfer best solution from previous graph

      Common adaptations:
      - Narrow bounds around promising region (improve efficiency)
      - Extend bounds if solution hits boundary
      - Note: bounds are defined at problem creation, NOT via sampler_options

  learning:
    track_options:
      - sampler
      - n_trials
      - n_startup_trials
      - pruner
    success_indicators:
      - trials_completed_vs_pruned_ratio
      - convergence_curve_shape
    failure_patterns:
      - pattern: "All trials pruned"
        likely_cause: "Pruner too aggressive or poor initial space"
        suggested_action: "Increase n_startup_trials or relax pruner"
      - pattern: "No improvement after n_startup_trials"
        likely_cause: "Wrong sampler for problem structure"
        suggested_action: "Try different sampler (TPEâ†’CMA-ES or vice versa)"
      - pattern: "Many trials at bounds"
        likely_cause: "Search space too narrow"
        suggested_action: "Extend bounds, consider graph node with wider space"
